{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    df_train = pd.read_pickle('../../data/new/train.pkl')\n",
    "    df_val = pd.read_pickle('../../data/new/val.pkl')\n",
    "    df_test = pd.read_pickle('../../data/new/test.pkl')\n",
    "    \n",
    "    print('Train: {} samples'.format(df_train.shape[0]))\n",
    "    print('Val: {} samples'.format(df_val.shape[0]))\n",
    "    print('Test: {} samples'.format(df_test.shape[0]))\n",
    "    \n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: fastText inspired model for tiltle and keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def concat_keyword_title(df):\n",
    "    df.keywords = df.keywords.apply(' ,'.join)\n",
    "    df['concat_keywords_title'] = df.keywords + df.title\n",
    "    \n",
    "    return df.concat_keywords_title.values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.replace(\"' \", \" ' \")\n",
    "    signs = set(',.:;\"?!')\n",
    "    prods = set(text) & signs\n",
    "    if not prods:\n",
    "        return text\n",
    "\n",
    "    for sign in prods:\n",
    "        text = text.replace(sign, ' {} '.format(sign) )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_docs(df, n_gram_max=2):\n",
    "    def add_ngram(q, n_gram_max):\n",
    "            ngrams = []\n",
    "            for n in range(2, n_gram_max+1):\n",
    "                for w_index in range(len(q)-n+1):\n",
    "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n",
    "            return q + ngrams\n",
    "        \n",
    "    docs = []\n",
    "    for doc in df:\n",
    "        doc = preprocess(doc).split()\n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenizer_title(docs,min_count=2, tokenizer=None):\n",
    "    '''\n",
    "    Args:\n",
    "        docs: list of texts, the first element must be the training data\n",
    "    '''\n",
    "    if tokenizer == None:\n",
    "        tokenizer = Tokenizer(lower=True, filters='')\n",
    "        tokenizer.fit_on_texts(docs[0])\n",
    "        num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "        \n",
    "        tokenizer = Tokenizer(num_words = num_words, lower=True, filters='')\n",
    "        tokenizer.fit_on_texts(docs[0])\n",
    "    \n",
    "    return [tokenizer.texts_to_sequences(doc) for doc in docs], tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def padding(docs, maxlen):\n",
    "    return [pad_sequences(sequences=doc, maxlen=maxlen) for doc in docs ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: LSTM for text inspired by BLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = '/Users/thesuguser/Desktop/kaggle/toxic_comment/embeddings/crawl-300d-2M.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding_matrix(embedding_file, max_features, embeded_size, tokenizer):\n",
    "    def get_coefs(word, *arr):\n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(embedding_file))\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.zeros((nb_words, embeded_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i>= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenizer_text(docs, max_features, maxlen, tokenizer=None,):\n",
    "    '''\n",
    "    args*:\n",
    "        docs: list of texts, the first element must be the training data\n",
    "    '''\n",
    "    if tokenizer==None:\n",
    "        tokenizer = Tokenizer(num_words=max_features)\n",
    "        tokenizer.fit_on_texts(docs[0])\n",
    "    docs = [tokenizer.texts_to_sequences(doc) for doc in docs]\n",
    "    \n",
    "    return [pad_sequences(doc, maxlen = maxlen) for doc in docs], tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for title and keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_count = 2\n",
    "maxlen_1 = 51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features_2 = 20000\n",
    "embed_size  = 300\n",
    "maxlen_2 = 155"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_train = concat_keyword_title(df_train)\n",
    "title_val = concat_keyword_title(df_val)\n",
    "title_test = concat_keyword_title(df_test)\n",
    "\n",
    "title_train = create_docs(title_train)\n",
    "title_val = create_docs(title_val)\n",
    "title_test = create_docs(title_test)\n",
    "\n",
    "[title_train, title_val, title_test], tokenizer_1 = tokenizer_title([title_train, title_val, title_test], min_count=min_count)\n",
    "title_train, title_val, title_test = padding([title_train, title_val, title_val],maxlen=maxlen_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_train = np.array(title_train)\n",
    "title_val = np.array(title_val)\n",
    "title_test = np.array(title_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_features_1 = np.max(title_train) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[text_train, text_val, text_test], tokenizer_2 = tokenizer_text([df_train.article.values,\n",
    "                                                  df_val.article.values,\n",
    "                                                  df_test.article.values],\n",
    "                                                max_features=max_features_2,\n",
    "                                                maxlen=maxlen_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_matrix = get_embedding_matrix(EMBEDDING_FILE, \n",
    "                                        max_features=max_features_2, \n",
    "                                        embeded_size=embed_size,\n",
    "                                        tokenizer=tokenizer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train, y_val, y_test = [pd.get_dummies(d.popularity).values for d in [df_train, df_val, df_test]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RocAucMetricCallback(Callback):\n",
    "    def __init__(self, predict_batch_size=1024, include_on_batch=False):\n",
    "        super(RocAucMetricCallback, self).__init__()\n",
    "        self.predict_batch_size=predict_batch_size\n",
    "        self.include_on_batch=include_on_batch\n",
    "    \n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        pass\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        if(self.include_on_batch):\n",
    "            logs['roc_auc_val']=float('-inf')\n",
    "            if(self.validation_data):\n",
    "                logs['roc_auc_val']=roc_auc_score(self.validation_data[2], \n",
    "                                                  self.model.predict({'text_input':self.validation_data[0],\n",
    "                                                                      'title_input':self.validation_data[1]},\n",
    "                                                                     batch_size=self.predict_batch_size))\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        if not ('roc_auc_val' in self.params['metrics']):\n",
    "            self.params['metrics'].append('roc_auc_val')\n",
    "    \n",
    "    def on_train_end(self, logs={}):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        pass\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        logs['roc_auc_val']=float('-inf')\n",
    "        if(self.validation_data):\n",
    "            score = roc_auc_score(self.validation_data[2], \n",
    "                                  self.model.predict({'text_input':self.validation_data[0],\n",
    "                                                      'title_input':self.validation_data[1]},\n",
    "                                                       batch_size=self.predict_batch_size))\n",
    "            logs['roc_auc_val']=score\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Computing the focal loss\n",
    "    \n",
    "    Args: \n",
    "        gamma: the tunable focusing parameter\n",
    "        alpha: the balance parameter\n",
    "    Return:\n",
    "        the value of the focal loss\n",
    "    \"\"\"\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute the focal loss with the prediction result and the ground truth\n",
    "        \n",
    "        Args:\n",
    "            y_true: the ground truth\n",
    "            y_pred: the probabilities of the prediction\n",
    "        \"\"\"\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(maxlen_1, maxlen_2, max_features_1, max_features_2):\n",
    "    \"\"\"\n",
    "    Construct the computational graph of the deep learning model\n",
    "    \n",
    "    Args:\n",
    "        maxlen_1: the maximum length of the title&keywords inputs\n",
    "        maxlen_2: the maximum length of the body text inputs\n",
    "        max_features_1: the maxmimum number of the feature vector of the title&keyword inputs\n",
    "        max_features_2: the maxmimum number of the feature vector of the body text inputs\n",
    "    Returns:\n",
    "        the keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    text_input = Input(shape=(maxlen_2,), dtype='int32', name='text_input')\n",
    "    # Embedding layer for body text\n",
    "    emb_1 = Embedding(max_features_2, output_dim=300, weights=[embedding_matrix])(text_input)\n",
    "    x1 = SpatialDropout1D(0.2)(emb_1)\n",
    "    # Feture extractor\n",
    "    x1 = Bidirectional(GRU(80, return_sequences=True))(x1)\n",
    "    avg_pool = GlobalAveragePooling1D()(x1)\n",
    "    max_pool = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    title_input = Input(shape=(maxlen_1,), dtype='int32', name='title_input')\n",
    "    # Embedding layer for title and keywords\n",
    "    emb_2 =Embedding(max_features_1, output_dim=20)(title_input)\n",
    "    # Feature extractor\n",
    "    x2 = GlobalAveragePooling1D()(emb_2)\n",
    "    \n",
    "    conc = concatenate([x2, avg_pool, max_pool])\n",
    "    \n",
    "    # Classifier\n",
    "    fc = Dense(64, activation='relu')(conc)\n",
    "    output = Dense(3, activation='softmax')(fc)\n",
    "    \n",
    "    model = Model(inputs=[text_input, title_input], outputs=output)\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss=[focal_loss(gamma=2., alpha=0.25)],\n",
    "                  optimizer = 'adam', \n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tbCallBack = TensorBoard(log_dir='output/', histogram_freq=0, write_graph=True, write_images=True)\n",
    "cb = [\n",
    "    RocAucMetricCallback(include_on_batch=False), # include it before EarlyStopping!\n",
    "    EarlyStopping(monitor='roc_auc_val',patience=5, verbose=2,mode='max'),\n",
    "    tbCallBack,\n",
    "    ModelCheckpoint('weights.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='roc_auc_val', verbose=1)    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hist = model.fit({'text_input':text_train, 'title_input':title_train}, \n",
    "                 y_train,\n",
    "                 batch_size = 128,\n",
    "                 validation_data = ([text_val, title_val],y_val),\n",
    "                 epochs = 100,\n",
    "                 callbacks = cb,\n",
    "                 verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
