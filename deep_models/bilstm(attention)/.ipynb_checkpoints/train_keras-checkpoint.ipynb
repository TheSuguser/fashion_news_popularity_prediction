{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import collections\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from data_utils import *\n",
    "#from attention_rnn import *\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(text):\n",
    "    \"\"\"\n",
    "    Clean the text\n",
    "    \n",
    "    Args:\n",
    "        original text\n",
    "    Returns:\n",
    "        cleaned text\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`\\\"]\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = text.strip().lower()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildWordDict(PATH):\n",
    "    \"\"\"\n",
    "    Build the word dictionary\n",
    "    \n",
    "    Args:\n",
    "        the path of the input\n",
    "    Returns:\n",
    "        word dictionary\n",
    "    \"\"\"\n",
    "\tif not os.path.exists(\"word_dict.pickle\"):\n",
    "\t\ttrain_df = pd.read_pickle(PATH)\n",
    "\t\tcontents = train_df['article']\n",
    "\n",
    "\t\twords = list()\n",
    "\t\tfor content in contents:\n",
    "\t\t\tfor word in word_tokenize(clean_str(content)):\n",
    "\t\t\t\twords.append(word)\n",
    "\n",
    "\t\tword_counter = collections.Counter(words).most_common()\n",
    "\t\tword_dict = dict()\n",
    "\t\tword_dict[\"<pad>\"] = 0\n",
    "\t\tword_dict[\"<unk>\"] = 1\n",
    "\t\tword_dict[\"<eos>\"] = 2\n",
    "\t\tfor word, _ in word_counter:\n",
    "\t\t\tword_dict[word] = len(word_dict)\n",
    "\n",
    "\t\twith open(\"word_dict.pickle\", \"wb\") as f:\n",
    "\t\t\tpickle.dump(word_dict,f)\n",
    "\n",
    "\telse:\n",
    "\t\twith open(\"word_dict.pickle\", \"rb\") as f:\n",
    "\t\t\tword_dict = pickle.load(f)\n",
    "\n",
    "\treturn word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_word_dataset(series, word_dict, max_len):\n",
    "\tseries = series.sample(frac=1)\n",
    "\tx = list(map(lambda d: word_tokenize(clean_str(d)), series))\n",
    "\tx = list(map(lambda d: list(map(lambda w: word_dict.get(w, word_dict[\"<unk>\"]), d)), x))\n",
    "\tx = list(map(lambda d: d + [word_dict[\"<eos>\"]], x))\n",
    "\tx = list(map(lambda d: d[:document_max_len], x))\n",
    "\tx = list(map(lambda d: d + (document_max_len - len(d)) * [word_dict[\"<pad>\"]], x))\n",
    "\t\n",
    "\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sequence_vectorize(train_texts, val_texts,test_texts, max_len, max_feature):\n",
    "    \"\"\"\n",
    "    Vectorize the text\n",
    "    \n",
    "    Args:\n",
    "        train_texts: training data\n",
    "        val_texts: validation data\n",
    "        test_texts: testing data\n",
    "        max_len: maximum length of the input\n",
    "        max_features: maximum length of the feature vector\n",
    "    Returns:\n",
    "        vectorized data and the tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = text.Tokenizer(num_words = max_feature)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
    "    x_test = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "    if len(max(x_train, key=len)) < max_len:\n",
    "        max_len = len(max(x_train, key=len))\n",
    "\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen= max_len)\n",
    "    x_val = sequence.pad_sequences(x_val, maxlen=max_len)\n",
    "    x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "    return x_train, x_val, x_test, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle('../../data/new/train.pkl')\n",
    "val_df = pd.read_pickle('../../data/new/val.pkl')\n",
    "test_df = pd.read_pickle('../../data/new/test.pkl')\n",
    "\n",
    "train_df['text'] = train_df.title +' '+ train_df.article\n",
    "val_df['text'] = val_df.title + ' '+ val_df.article\n",
    "test_df['text'] = test_df.title + ' ' + test_df.article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 131\n",
    "MAX_FEATURE = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_val,x_test, word_dict = sequence_vectorize(train_df.text.values, \n",
    "                                               val_df.text.values,\n",
    "                                               test_df.text.values,\n",
    "                                               MAX_LEN,\n",
    "                                               MAX_FEATURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = pd.get_dummies(train_df.popularity).values\n",
    "y_val = pd.get_dummies(val_df.popularity).values\n",
    "y_test = pd.get_dummies(test_df.popularity).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCOB_SIZE = len(word_dict)\n",
    "BATCH_SIZE = 128\n",
    "#EVALUATE_EVERY = 100\n",
    "#CHECKPOINT_EVERY = 100\n",
    "EPOCHES = 100\n",
    "#learning_rate = 1e-3\n",
    "EMBED_SIZE = 256\n",
    "NUM_HIDDEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Flatten\n",
    "from keras.layers import Input, Dense, Embedding, concatenate, Dropout\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, Callback, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class RocAucMetricCallback(Callback):\n",
    "    \"\"\"\n",
    "    Define a new callback to compute the roc auc score during the training process\n",
    "    \"\"\"\n",
    "    def __init__(self, predict_batch_size=1024, include_on_batch=False):\n",
    "        super(RocAucMetricCallback, self).__init__()\n",
    "        self.predict_batch_size=predict_batch_size\n",
    "        self.include_on_batch=include_on_batch\n",
    " \n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        pass\n",
    " \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        if(self.include_on_batch):\n",
    "            logs['roc_auc_val']=float('-inf')\n",
    "            if(self.validation_data):\n",
    "                logs['roc_auc_val']=roc_auc_score(self.validation_data[1], \n",
    "                                                  self.model.predict(self.validation_data[0],\n",
    "                                                                     batch_size=self.predict_batch_size))\n",
    " \n",
    "    def on_train_begin(self, logs={}):\n",
    "        if not ('roc_auc_val' in self.params['metrics']):\n",
    "            self.params['metrics'].append('roc_auc_val')\n",
    " \n",
    "    def on_train_end(self, logs={}):\n",
    "        pass\n",
    " \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        pass\n",
    " \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        logs['roc_auc_val']=float('-inf')\n",
    "        if(self.validation_data):\n",
    "            score = roc_auc_score(self.validation_data[1], \n",
    "                                              self.model.predict(self.validation_data[0],\n",
    "                                                                 batch_size=self.predict_batch_size))\n",
    "            logs['roc_auc_val']=score\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    " \n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    " \n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    " \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    " \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    " \n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    " \n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    " \n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    " \n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    " \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    " \n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    " \n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    " \n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    " \n",
    "        a = K.exp(ait)\n",
    " \n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    " \n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    " \n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tbCallBack = TensorBoard(log_dir='../../output/bilstm', histogram_freq=0, write_graph=True, write_images=True)\n",
    "cb = [\n",
    "    RocAucMetricCallback(), # include it before EarlyStopping!\n",
    "    EarlyStopping(monitor='roc_auc_val',patience=5, verbose=2,mode='max'),\n",
    "    tbCallBack,\n",
    "    ModelCheckpoint('weights.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='roc_auc_val', verbose=1)    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keras Model\n",
    "def get_model(attention=True):\n",
    "    \"\"\"\n",
    "    Construct the computational graph\n",
    "    \n",
    "    Args:\n",
    "        attention: if use the attention mechanism or not\n",
    "    Returns:\n",
    "        Keras implemented model\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_FEATURE, EMBED_SIZE, embeddings_initializer='uniform'))\n",
    "    if attention:\n",
    "        model.add(Bidirectional(LSTM(units=NUM_HIDDEN, dropout=0.5, return_sequences=True)))\n",
    "        model.add(Bidirectional(LSTM(units=NUM_HIDDEN, dropout=0.5)))\n",
    "        model.add(Dropout(0.5))\n",
    "    else:\n",
    "        model.add(Bidirectional(LSTM(units=NUM_HIDDEN, dropout=0.5, return_sequences=True)))\n",
    "        model.add(Bidirectional(LSTM(units=NUM_HIDDEN, dropout=0.5, rerurn_sequences=True)))\n",
    "        model.add(AttentionWithContext())\n",
    "    #model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    \n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 256)         5120000   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, None, 512)         1050624   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 512)               1574912   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 7,747,075\n",
      "Trainable params: 7,747,075\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74996 samples, validate on 8333 samples\n",
      "Epoch 1/100\n",
      "74996/74996 [==============================] - 226s 3ms/step - loss: 0.4763 - acc: 0.7493 - val_loss: 0.4515 - val_acc: 0.7763\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.778054 \n",
      "\n",
      "\n",
      "Epoch 00001: saving model to weights.01-0.45.hdf5\n",
      "Epoch 2/100\n",
      "74996/74996 [==============================] - 224s 3ms/step - loss: 0.4170 - acc: 0.7964 - val_loss: 0.4355 - val_acc: 0.7784\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.786083 \n",
      "\n",
      "\n",
      "Epoch 00002: saving model to weights.02-0.44.hdf5\n",
      "Epoch 3/100\n",
      "74996/74996 [==============================] - 224s 3ms/step - loss: 0.3705 - acc: 0.8265 - val_loss: 0.4482 - val_acc: 0.7765\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.780753 \n",
      "\n",
      "\n",
      "Epoch 00003: saving model to weights.03-0.45.hdf5\n",
      "Epoch 4/100\n",
      "74996/74996 [==============================] - 224s 3ms/step - loss: 0.3239 - acc: 0.8549 - val_loss: 0.4895 - val_acc: 0.7728\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.768836 \n",
      "\n",
      "\n",
      "Epoch 00004: saving model to weights.04-0.49.hdf5\n",
      "Epoch 5/100\n",
      "74996/74996 [==============================] - 224s 3ms/step - loss: 0.2724 - acc: 0.8822 - val_loss: 0.5562 - val_acc: 0.7612\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.758676 \n",
      "\n",
      "\n",
      "Epoch 00005: saving model to weights.05-0.56.hdf5\n",
      "Epoch 6/100\n",
      "74996/74996 [==============================] - 225s 3ms/step - loss: 0.2274 - acc: 0.9050 - val_loss: 0.6530 - val_acc: 0.7609\n",
      "\n",
      " ROC-AUC - epoch: 6 - score: 0.751912 \n",
      "\n",
      "\n",
      "Epoch 00006: saving model to weights.06-0.65.hdf5\n",
      "Epoch 7/100\n",
      "74996/74996 [==============================] - 224s 3ms/step - loss: 0.1869 - acc: 0.9241 - val_loss: 0.7647 - val_acc: 0.7591\n",
      "\n",
      " ROC-AUC - epoch: 7 - score: 0.736891 \n",
      "\n",
      "\n",
      "Epoch 00007: saving model to weights.07-0.76.hdf5\n",
      "Epoch 00007: early stopping\n",
      "Training time: 2493.1315\n"
     ]
    }
   ],
   "source": [
    "start = time.clock()\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size = BATCH_SIZE,\n",
    "          epochs = EPOCHES,\n",
    "          validation_data = (x_val, y_val),\n",
    "          callbacks=cb,\n",
    "          verbose=1)\n",
    "\n",
    "print(\"Training time:\" , time.clock() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_test = model.predict(x_test, 1024)\n",
    "pred_train = model.predict(x_train, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('pred_test.npy', pred_test)\n",
    "np.save('pred_train.npy',pred_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
