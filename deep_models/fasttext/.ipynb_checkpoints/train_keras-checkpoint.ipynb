{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import Callback, TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../../data/new/train.pkl')\n",
    "val = pd.read_pickle('../../data/new/val.pkl')\n",
    "test = pd.read_pickle('../../data/new/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.article = train.title + ' ' + train.article\n",
    "val.article = val.title + ' ' + val.article\n",
    "test.article = test.title + ' ' + test.article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Clean the text\n",
    "    \n",
    "    Args:\n",
    "        text: the original text\n",
    "    Returns:\n",
    "        text: the cleaned text\n",
    "    \"\"\"\n",
    "    text = text.replace(\"' \", \" ' \")\n",
    "    signs = set(',.:;\"?!')\n",
    "    prods = set(text) & signs\n",
    "    if not prods:\n",
    "        return text\n",
    "\n",
    "    for sign in prods:\n",
    "        text = text.replace(sign, ' {} '.format(sign) )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_docs(df, n_gram_max=2):\n",
    "    \"\"\"\n",
    "    Convert the origina text to the combination of unigrams and bigrams\n",
    "    \n",
    "    Args:\n",
    "        df: Dataframe \n",
    "        n_gram_max: the maximum of the n-grams kept in the data\n",
    "    Returns:\n",
    "        Combination of unigram and bigram\n",
    "    \"\"\"\n",
    "    def add_ngram(q, n_gram_max):\n",
    "            ngrams = []\n",
    "            for n in range(2, n_gram_max+1):\n",
    "                for w_index in range(len(q)-n+1):\n",
    "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n",
    "            return q + ngrams\n",
    "        \n",
    "    docs = []\n",
    "    for doc in df:\n",
    "        doc = preprocess(doc).split()\n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_docs = create_docs(train.article.values)\n",
    "val_docs = create_docs(val.article.values)\n",
    "test_docs = create_docs(test.article.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_count = 2\n",
    "tokenizer = Tokenizer(lower=True, filters='')\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    "num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words, lower=True, filters='')\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    "train_docs = tokenizer.texts_to_sequences(train_docs)\n",
    "val_docs = tokenizer.texts_to_sequences(val_docs)\n",
    "test_docs = tokenizer.texts_to_sequences(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371.88590058136435\n"
     ]
    }
   ],
   "source": [
    "# print(np.mean([len(doc) for doc in train_docs]))\n",
    "print(np.mean([len(doc) for doc in train_docs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 371\n",
    "x_train = pad_sequences(sequences=train_docs, maxlen=maxlen)\n",
    "x_val = pad_sequences(sequences=val_docs, maxlen=maxlen)\n",
    "x_test = pad_sequences(sequences=test_docs, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = pd.get_dummies(train.popularity).values\n",
    "y_val = pd.get_dummies(val.popularity).values\n",
    "y_test = pd.get_dummies(test.popularity).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "class RocAucMetricCallback(Callback):\n",
    "    \"\"\"\n",
    "    Define a new callback to compute the roc auc score during the training process\n",
    "    \"\"\"\n",
    "    def __init__(self, predict_batch_size=1024, include_on_batch=False):\n",
    "        super(RocAucMetricCallback, self).__init__()\n",
    "        self.predict_batch_size=predict_batch_size\n",
    "        self.include_on_batch=include_on_batch\n",
    " \n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        pass\n",
    " \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        if(self.include_on_batch):\n",
    "            logs['roc_auc_val']=float('-inf')\n",
    "            if(self.validation_data):\n",
    "                logs['roc_auc_val']=roc_auc_score(self.validation_data[1], \n",
    "                                                  self.model.predict(self.validation_data[0],\n",
    "                                                                     batch_size=self.predict_batch_size))\n",
    " \n",
    "    def on_train_begin(self, logs={}):\n",
    "        if not ('roc_auc_val' in self.params['metrics']):\n",
    "            self.params['metrics'].append('roc_auc_val')\n",
    " \n",
    "    def on_train_end(self, logs={}):\n",
    "        pass\n",
    " \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        pass\n",
    " \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        logs['roc_auc_val']=float('-inf')\n",
    "        if(self.validation_data):\n",
    "            score = roc_auc_score(self.validation_data[1], \n",
    "                                              self.model.predict(self.validation_data[0],\n",
    "                                                                 batch_size=self.predict_batch_size))\n",
    "            logs['roc_auc_val']=score\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/miniconda3/envs/dl/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "tbCallBack = TensorBoard(log_dir='../../output/fasttext', histogram_freq=0, write_graph=True, write_images=True)\n",
    "cb = [\n",
    "    RocAucMetricCallback(), # include it before EarlyStopping!\n",
    "    EarlyStopping(monitor='roc_auc_val',patience=5, verbose=2,mode='max'),\n",
    "    tbCallBack,\n",
    "    ModelCheckpoint('weights.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='roc_auc_val', verbose=1)    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = np.max(x_train) + 1\n",
    "embedding_dims = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(input_dim, embedding_dims=20, optimizer='adam'):\n",
    "    \"\"\"\n",
    "    Construct the computational graph of the fasttext\n",
    "    \n",
    "    Args:\n",
    "        input_dim: the dimension of the input vector\n",
    "        embedding_dims: the dimension of the embedding layer\n",
    "        optimizer: the optimizer used to optimize the loss function\n",
    "    Returns:\n",
    "        The Keras implemented model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = input_dim, output_dim=embedding_dims))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(3,activation='softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer = optimizer, \n",
    "                  metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 20)          21204700  \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 21,204,763\n",
      "Trainable params: 21,204,763\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model(input_dim=input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74996 samples, validate on 8333 samples\n",
      "Epoch 1/100\n",
      "74996/74996 [==============================] - 14s 190us/step - loss: 0.8384 - acc: 0.5865 - val_loss: 0.7654 - val_acc: 0.6328\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.745814 \n",
      "\n",
      "\n",
      "Epoch 00001: saving model to weights.01-0.77.hdf5\n",
      "Epoch 2/100\n",
      "74996/74996 [==============================] - 14s 188us/step - loss: 0.6979 - acc: 0.7000 - val_loss: 0.7059 - val_acc: 0.6786\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.797263 \n",
      "\n",
      "\n",
      "Epoch 00002: saving model to weights.02-0.71.hdf5\n",
      "Epoch 3/100\n",
      "74996/74996 [==============================] - 13s 180us/step - loss: 0.5736 - acc: 0.7817 - val_loss: 0.6678 - val_acc: 0.6959\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.822195 \n",
      "\n",
      "\n",
      "Epoch 00003: saving model to weights.03-0.67.hdf5\n",
      "Epoch 4/100\n",
      "74996/74996 [==============================] - 14s 188us/step - loss: 0.4438 - acc: 0.8462 - val_loss: 0.6550 - val_acc: 0.7049\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.829257 \n",
      "\n",
      "\n",
      "Epoch 00004: saving model to weights.04-0.65.hdf5\n",
      "Epoch 5/100\n",
      "74996/74996 [==============================] - 14s 190us/step - loss: 0.3240 - acc: 0.9016 - val_loss: 0.6574 - val_acc: 0.7038\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.830785 \n",
      "\n",
      "\n",
      "Epoch 00005: saving model to weights.05-0.66.hdf5\n",
      "Epoch 6/100\n",
      "74996/74996 [==============================] - 14s 189us/step - loss: 0.2233 - acc: 0.9429 - val_loss: 0.6747 - val_acc: 0.7002\n",
      "\n",
      " ROC-AUC - epoch: 6 - score: 0.829388 \n",
      "\n",
      "\n",
      "Epoch 00006: saving model to weights.06-0.67.hdf5\n",
      "Epoch 7/100\n",
      "74996/74996 [==============================] - 14s 187us/step - loss: 0.1468 - acc: 0.9703 - val_loss: 0.7051 - val_acc: 0.6937\n",
      "\n",
      " ROC-AUC - epoch: 7 - score: 0.825295 \n",
      "\n",
      "\n",
      "Epoch 00007: saving model to weights.07-0.71.hdf5\n",
      "Epoch 8/100\n",
      "74996/74996 [==============================] - 14s 185us/step - loss: 0.0931 - acc: 0.9863 - val_loss: 0.7367 - val_acc: 0.6935\n",
      "\n",
      " ROC-AUC - epoch: 8 - score: 0.821719 \n",
      "\n",
      "\n",
      "Epoch 00008: saving model to weights.08-0.74.hdf5\n",
      "Epoch 9/100\n",
      "74996/74996 [==============================] - 14s 189us/step - loss: 0.0578 - acc: 0.9935 - val_loss: 0.7830 - val_acc: 0.6924\n",
      "\n",
      " ROC-AUC - epoch: 9 - score: 0.818743 \n",
      "\n",
      "\n",
      "Epoch 00009: saving model to weights.09-0.78.hdf5\n",
      "Epoch 10/100\n",
      "74996/74996 [==============================] - 14s 188us/step - loss: 0.0357 - acc: 0.9967 - val_loss: 0.8326 - val_acc: 0.6904\n",
      "\n",
      " ROC-AUC - epoch: 10 - score: 0.815812 \n",
      "\n",
      "\n",
      "Epoch 00010: saving model to weights.10-0.83.hdf5\n",
      "Epoch 00010: early stopping\n",
      "Training time: 201.387539\n"
     ]
    }
   ],
   "source": [
    "start = time.clock()\n",
    "hist = model.fit(x_train, \n",
    "                 y_train,\n",
    "                 batch_size = 64,\n",
    "                 validation_data = (x_val,y_val),\n",
    "                 epochs = 100,\n",
    "                 callbacks = cb,\n",
    "                 verbose=1)\n",
    "print('Training time:', time.clock() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_train = model.predict(x_train, batch_size=1024)\n",
    "pred_test = model.predict(x_test, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('pred_train.npy',pred_train)\n",
    "np.save('pred_test.npy', pred_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
